# Ray Serve LLM Requirements
# Note: Ray is already installed in /opt/ray

# vLLM backend for LLM serving
vllm>=0.10.1

# OpenAI-compatible API client (for testing)
openai>=1.0.0

# Additional dependencies that may be needed
transformers>=4.30.0
torch>=2.0.0
